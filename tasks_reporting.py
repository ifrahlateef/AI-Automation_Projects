# In[1]:
# Loading libraries
import os
import sys
print('**************************')
print('Generated by ', os.getlogin())
print('**************************')


# In[2]:


repo_dir='C:\\Dev\\repository_python'


# In[3]:


business_group_string="Business_group"


csg_string="CSG"
corporate_string="Corporate"

csg_tasks_locations=['UK/Cardiff','UK/Glasgow/City Park','UK/Leeds/Central Square','UK/Leeds/City Walk',
                         'UK/Sheffield','UK/Stockport','UK/Newcastle/Wellbar Central','UK/Uddingston',
                        'UK/Dunfermline']

not_categorised_string='Not Categorised'


# In[4]:


#debug_outputs_dir="debug_outputs/"


# In[ ]:


debug_outputs_dir="debug_outputs/"





# In[5]:To run the file on Server
    
debug_print_excel=False
use_debug_output_dir=False

if(use_debug_output_dir):
    data_warehouse_dir_path="C:\\inetpub\\wwwroot\\gws\\dts_activity_dbg\\files\\"
else:
    data_warehouse_dir_path="C:\\inetpub\\wwwroot\\gws\\dts_activity\\files\\"


filename_tasks='dts_reporting_tasks'


def print_excel(df,filepath):
    global debug_print_excel

    if(debug_print_excel):
        df.to_excel(filepath)

def print_output(df,fase_nr):
    global debug_outputs_dir
    
    print_excel(df,debug_outputs_dir+str(fase_nr)+".xlsx")

    
# In[6]:


if repo_dir not in sys.path:
    sys.path.insert(0, repo_dir)


# In[7]:
# Importing my package of libraries

from load_excel import *
from data_warehouse_connection import *
from data_sources import *
from pythonrepo_modules import *


# In[8]:


# imports for database handling
import pyodbc
import sqlalchemy as sa
import urllib
import pandas as pd
from IPython.display import Latex
import numpy as np
from datetime import datetime


# In[9]:


def check_result(result):
    for ii in result:
        print(ii)


# In[10]:


def assign_business_group(x):
    result=corporate_string

    ass_group=x.dv_assignment_group
    
    if(ass_group=='UK CSG Desktop Support'):
        return csg_string
    if(ass_group == 'UK Desktop Support'):
         if(str(x.user_location).startswith(tuple(csg_tasks_locations))):
            return csg_string
    return result


# In[11]:


not_categorised_requests=["UK RITM - Ipad Mgmt Request", "Tech-Store - Non catalog HW Item", "UK RITM - No app+2Task (incident Hardware refresh)", 
                            "UK RITM - AdHoc with Manager Approval", "UK RITM - Ad Hoc WF", "UK RITM - Restricted Application Account", 
                            "Tech-Store - ITP Order Management"]
#not_categorised_requests=[x.lower() for x in not_categorised_requests]

def Auto_classification_type(x):
    classification_str=x.dv_workflow
    if(classification_str != ''):
        if(str(x.dv_workflow).startswith(tuple(not_categorised_requests))):
        #if(classification_str not in not_categorised_requests):
            return('Not classified')     
        else:
            return('Classified')
            


# In[12]:


#### Eastablishing connection to spark
engine1=connect_to_spark()

with engine1.connect() as con:
    query='use database_name;'
    aa=con.execute(query)


    query_names="""
    DROP TABLE if exists ##tmp_sctask1;
    """
    bb=con.execute(query_names)

    query_names="""
    select number, dv_request, dv_request_item, dv_short_description, description, dv_u_completion_method, dv_u_completion_category, dv_state, sys_created_on, closed_at, dv_assignment_group
    into ##tmp_sctask1
    from
    [~sc_task]
    WHERE
    sys_created_on > convert(datetime,'2022-10-01 00:00:00',120)
    and
    dv_assignment_group in ('UK Desktop Support','UK CSG Desktop Support');
    """
    bb=con.execute(query_names)
 
    query_names="""
    DROP TABLE if exists ##tmp_req_item1;
    """
    bb=con.execute(query_names)

    query_names="""
    select req.dv_cat_item,req.cat_item,req.quantity,task.*
    into ##tmp_req_item1
    from
    ##tmp_sctask1 task
    left join
    [~sc_req_item] req
    on
    req.number=task.dv_request_item
    ;
    """
    bb=con.execute(query_names)

    query_names="""
    DROP TABLE if exists ##tmp_catit1;
    """
    bb=con.execute(query_names)

    query_names="""
    select catit.dv_sc_catalogs,catit.dv_workflow,catit.model, req.*
    into ##tmp_catit1
    from
    [~sc_cat_item] catit
    left join
    ##tmp_req_item1 req
    on
    catit.sys_id=req.cat_item
    ;
    """
    bb=con.execute(query_names)
    
    query_names="""
    DROP TABLE if exists ##tmp_model1;
    """
    bb=con.execute(query_names)
    
    query_names="""
    select dv_u_model_type,dv_cmdb_model_category,catit.*
    into ##tmp_model1
    from ##tmp_catit1 catit
    left join
    [~cmdb_model] model
    on
    model.sys_id=model
    ;
    """
    bb=con.execute(query_names)

    query_names="""
    DROP TABLE if exists ##tmp_request1;
    """
    bb=con.execute(query_names)

    query_names="""
    select req.requested_for,req.dv_requested_for,tpmodel.*
    into ##tmp_request1
    from ##tmp_model1 tpmodel
    left join
    [~sc_request] req
    on
    req.number=tpmodel.dv_request
    ;
    """
    bb=con.execute(query_names)

    query_names="""
    DROP TABLE if exists ##tmp_final1;
    """
    bb=con.execute(query_names)

    query_names="""
    select usr.dv_location user_location,req.*
    into ##tmp_final1
    from
    ##tmp_request1 req
    left join
    [~sys_user] usr
    on
    req.requested_for=usr.sys_id
    ;
    """
    bb=con.execute(query_names)
 
    query_names="""
    select * from
    ##tmp_final1
    where
    ( -- DTS
    (
    sys_created_on > convert(datetime,'2022-10-01 00:00:00',120)
    or
    closed_at > convert(datetime,'2022-10-01 00:00:00',120)
    or
    closed_at is null
    )
    and
    dv_assignment_group ='UK Desktop Support'
    )



    or



    ( -- CSG



    (-- done
    dv_assignment_group ='UK CSG Desktop Support'
    and
    (
    sys_created_on > convert(datetime,'2022-10-01 00:00:00',120)
    or
    closed_at > convert(datetime,'2022-10-01 00:00:00',120)
    or
    closed_at is null
    )
    )



    or



    (
    (
    dv_assignment_group ='UK Desktop Support'
    and
    sys_created_on >= convert(datetime,'2022-10-01 00:00:00',120)
    and
    (
    user_location like 'UK/Cardiff%'
    or
    user_location like 'UK/Glasgow/City Park%'
    or
    user_location like 'UK/Leeds/Central Square%'
    or
    user_location like 'UK/Leeds/City Walk%'
    or
    user_location like 'UK/Sheffield%'
    or
    user_location like 'UK/Stockport%'
    or
    user_location like 'UK/Newcastle/Wellbar Central%'
    or
    user_location like 'UK/Uddingston%'
    or
    user_location like 'UK/Dunfermline%'
    )
    )
    )
    );
    """
    df_data_extract=pd.read_sql(query_names, con) 


    
    
    
print("Length of data extract: ",len(df_data_extract))





#print_output(df=df_data_extract,fase_nr=1)

# In[13]:


#print_output(df=df_data_extract,phase_nr=1)


# In[14]:


df_data_extract.columns


# In[15]:


columns_subset_extract=['number', 'user_location', 'dv_u_model_type', 'dv_workflow', 'quantity', 'dv_request',
                        'dv_request_item', 'dv_short_description', 'description', 'dv_u_completion_method', 
                        'dv_u_completion_category', 'dv_state', 'closed_at', 'dv_assignment_group']


# In[16]:


df_data_extract=df_data_extract[columns_subset_extract]


# In[17]:


df_data_extract=df_data_extract.fillna('')


# In[18]:


### to remove time from closed_at and just fetch the date
#df_data_extract['closed_date'] = df_data_extract['closed_date'].dt.strttime('%Y-%m-%d')
df_data_extract['closed_date'] = pd.DatetimeIndex(df_data_extract['closed_at']).date
df_data_extract


# In[19]:


df_data_extract=df_data_extract.replace({pd.NaT: '', np.NaN: ''})


# In[20]:


### Calling the function for the Business group class
df_data_extract[business_group_string]=df_data_extract.apply(lambda x: assign_business_group(x), axis=1)


# In[21]:


### To check if the assigned classifications return any exceptions
df_data_extract['assign_classification']=df_data_extract.apply(lambda x: Auto_classification_type(x),axis=1)


# In[22]:


### Fetching the data when the 'state' of TASK is 'Complete' and 'completion category' is not cancelled later 
df_data_extract = df_data_extract.loc[(df_data_extract["dv_state"] == 'Complete') & (df_data_extract["dv_u_completion_category"] != 'Cancelled')]
df_data_extract.shape


# In[23]:


### In historic data few fields has missing values in completion method, so here override them with 'Engineer - Desktop'
df_data_extract = df_data_extract.replace(to_replace={'dv_u_completion_method': {'': 'Engineer - Desktop'}})


# In[24]:


df = df_data_extract


# In[25]:


### Defining a function to avoid duplication of TASK one in corporate and again in CSG
### Rather if SCTASKS gets assigned to CSG then that request containing one or many TASKS will be assigned to CSG
def update_business_group(group):
    if 'CSG' in group['Business_group'].values:
        group['Business_group'] = 'CSG'
    return group

df = df.groupby('dv_request').apply(update_business_group).reset_index(drop=True)
df


# In[26]:


#df.to_excel('fileout.xlsx')


# In[27]:


### To print the date column in yearmonth format and handle any missing dates
#df['date_column'] = df['date_column'].dt.strftime('%Y-%m-%d')
#df['closed_date']=pd.to_datetime(df['closed_date'])
#df['closed_date'] = df['closed_date'].dt.strttime('%Y-%m-%d')
df['closed_date'] = pd.DatetimeIndex(df['closed_at']).date
df['yearmonth']=df['closed_date'].apply(lambda x: "{:04d}".format(int(x.year))+"{:02d}".format(int(x.month)) if isinstance(x, datetime) else ('190001' if (isinstance(x, str) and x=='') else "{:04d}".format(int(pd.to_datetime(x).year))+"{:02d}".format(int(pd.to_datetime(x).month)) )  )
df


# In[28]:


#df.to_excel('fileout_C.xlsx')


# ### Data frame(df1) contains all SCTASKS related to Service Requests

# In[29]:


### df1 - data frame consist of SCTASK related to Generic Request Activities
df1 = df.loc[df["dv_workflow"] != "Tech-Store - Standard Hardware Item"]
df1.shape


# ### Data frame(df2) contains all SCTASKS related to Tech-Store orders

# In[30]:


### df2 - dataframe consist of SCTASK related to Tech-Store()
df2 = df.loc[df["dv_workflow"] == "Tech-Store - Standard Hardware Item"]
df2.shape


# In[31]:

### New addition for Tech Vend Onsite where in the 'description' it contains 'Tech Vend' we categorise them as 'Tech Vend Onsite' and the logic will be all unique dates will be given an activity point and occurences of same dates will be set to duplicates

selected_rows_techvend = df2['description'].str.contains('tech vend', case=False)
TV_df = df2.loc[selected_rows_techvend]
TV_df.shape
TV_df['description'] =  TV_df['description'].str.lower()
df2.drop(index=TV_df.index, inplace=True)
df2.shape

TV_df['closed_date'] = pd.DatetimeIndex(TV_df['closed_at']).date
TV_df

# Reset the index
TV_df = TV_df.reset_index(drop=True)
# Initialize the unique_dates dictionary with all req values
unique_dates = {}
for req in set(TV_df['dv_request']):
    unique_dates[req] = set()

# Store the unique dates for each req
for i, req in enumerate(TV_df['dv_request']):
    if TV_df['dv_state'][i] == 'Cancelled':
        continue
    date = TV_df['closed_date'][i]
    unique_dates[req].add(date)
    
# Create the output list
activity_count_output = []
classification_type_output = []
for i, req in enumerate(TV_df['dv_request']):
    if TV_df['dv_state'][i] == 'Cancelled':
        activity_count_output.append('')
        classification_type_output.append('Cancelled')
        #classification_type_output('Cancelled')
        #output.append('')
        continue
    date = TV_df['closed_date'][i]
    if date in unique_dates[req]:
        activity_count_output.append('1')
        unique_dates[req].remove(date)
        
        
        if 'tech vend' in TV_df['description'][i]:
            classification_type_output.append('Tech Vend Onsite')
        else:
            classification_type_output.append('Exception')
    else:
        activity_count_output.append('')
        classification_type_output.append('Duplicate')
        

df_TV = pd.DataFrame({'number': TV_df['number'], 'user_location': TV_df['user_location'],
                      'dv_u_model_type': TV_df['dv_u_model_type'], 'dv_workflow': TV_df['dv_workflow'],
                      'quantity': TV_df['quantity'], 'dv_request': TV_df['dv_request'],
                      'dv_request_item': TV_df['dv_request_item'], 'dv_short_description': TV_df['dv_short_description'],
                      'description': TV_df['description'], 'dv_u_completion_method': TV_df['dv_u_completion_method'],
                      'dv_u_completion_category': TV_df['dv_u_completion_category'],'dv_state': TV_df['dv_state'], 'closed_at': TV_df['closed_at'],
                      'dv_assignment_group': TV_df['dv_assignment_group'], 'closed_date': TV_df['closed_date'],
                      'Business_group': TV_df['Business_group'], 'assign_classification': TV_df['assign_classification'], 
                      'yearmonth': TV_df['yearmonth'], 
                      'Auto_Classification_type': classification_type_output, 'Activity_count': activity_count_output})
                      

df_TV



selected_rows_genesys = df2['dv_short_description'].str.contains('DTS Genesys')
DTS_df = df2.loc[selected_rows_genesys]
DTS_df.shape


# In[32]:


DTS_df['closed_date'] = pd.DatetimeIndex(DTS_df['closed_at']).date


# In[33]:


DTS_df['Auto_Classification_type'] = 'DTS Genesys'
DTS_df


# In[34]:


Genesys = DTS_df['Auto_Classification_type'].str.startswith("DTS Genesys")
DTS_df.loc[Genesys, 'Activity_count'] = str(1)


# In[35]:


df2.drop(index=DTS_df.index, inplace=True)
df2.shape


# In[36]:


#df1['closed_date'] = pd.DatetimeIndex(df1['closed_at']).date


# In[37]:


selected_rows_OM = df2['dv_short_description'].str.contains('Order Management')
OM_df = df2.loc[selected_rows_OM]
OM_df.shape


# In[38]:


OM_df['closed_date'] = pd.DatetimeIndex(OM_df['closed_at']).date


# In[39]:


OM_df['Auto_Classification_type'] = 'Exception'
OM_df


# In[40]:


df2.drop(index=OM_df.index, inplace=True)
df2.shape


# In[41]:
combined_df = pd.concat([df_TV, DTS_df, OM_df], ignore_index=True)
combined_df

df1['closed_date'] = pd.DatetimeIndex(df1['closed_at']).date


# ### Classification Logic for Standard Generic tasks (1:1)
# | dv_workflow                 | Classification_type (tmp col created to store return values) |
# |--------------------------|------------------------------|
# | Accessibility Request      | Accessibility Request         |
# | UK RITM - DTS Leaver (no approval) | Leaver DTS Record   |
# | Tech-Store - Non Catalog SW Item  | Software          |
# | Tech-Store Standard Software Item v3 | Software |
# | UK RITM - New Software Request | Software |
# | UK RITM - Software License Key Issue  | software |

# In[42]:


df1.loc[(df1['dv_workflow'].astype(str).str.startswith("Accessibility Request")),'Classification_type']= 'Accessibility Request'

#df1.loc[(df1['dv_workflow'].astype(str).str.startswith("UK RITM - Ipad Mgmt Request")),'Classification_type']= 'UK RITM - Ipad Mgmt Request'

df1.loc[(df1['dv_workflow'].astype(str).str.startswith("UK RITM - DTS Leaver (no approval)")), 'Classification_type']= 'Leaver DTS Record'

df1.loc[(df1['dv_workflow'].astype(str).str.startswith("Tech-Store - Non Catalog SW Item")), 'Classification_type'] = 'Software'

df1.loc[(df1['dv_workflow'].astype(str).str.startswith("Tech-Store Standard Software Item v3")), 'Classification_type'] = 'Software'

df1.loc[(df1['dv_workflow'].astype(str).str.startswith("UK RITM - New Software Request")), 'Classification_type'] = 'Software'

df1.loc[(df1['dv_workflow'].astype(str).str.startswith("UK RITM - Software License Key Issue")), 'Classification_type'] = 'Software'

df1


# ### Generated Activity count for each tasks
# | dv_workflow                 | Accessibility Request (tmp col created to store return values) |
# |--------------------------|------------------------------|
# | Accessibility Request      | 1         |
# 
# | dv_workflow                 | UK RITM - DTS Leaver (tmp col created to store return values) |
# |--------------------------|------------------------------|
# | UK RITM - DTS Leaver (no approval)      | 1       |
# 
# | dv_workflow                 | Software_Bundle (tmp col created to store return values) |
# |--------------------------|------------------------------|
# | Tech-Store - Non Catalog SW Item      | 1       |
# | Tech-Store Standard Software Item v3      | 1       |
# | UK RITM - New Software Request | 1     |
# | UK RITM - Software License Key Issue | 1     |
# 
# 
# 

# In[43]:


###### This to to generate the count of the tasks
Access = df1['dv_workflow'].str.startswith("Accessibility Request")
df1.loc[Access, 'Accessibility_Request'] = str(1)

#Ipad = df1['dv_workflow'].str.startswith("UK RITM - Ipad Mgmt Request"); now classified as exception
#df1.loc[Ipad, 'UK_RITM_Ipad_Mgmt'] = str(1)

Leaver = df1['dv_workflow'].str.startswith("UK RITM - DTS Leaver (no approval)")
df1.loc[Leaver, 'UK RITM - DTS Leaver'] = str(1)

Software1 = df1['dv_workflow'].str.startswith("Tech-Store - Non Catalog SW Item")
df1.loc[Software1, 'Software_Bundle'] = str(1)

Software2 = df1['dv_workflow'].str.startswith("Tech-Store Standard Software Item v3")
df1.loc[Software2, 'Software_Bundle'] = str(1)

Software3 = df1['dv_workflow'].str.startswith("UK RITM - New Software Request")
df1.loc[Software3, 'Software_Bundle'] = str(1)

Software4 = df1['dv_workflow'].str.startswith("UK RITM - Software License Key Issue")
df1.loc[Software4, 'Software_Bundle'] = str(1)

df1


# ### Loan Laptop Classification Table
# | dv_workflow                 | dv_short_description | col0 (tmp col created to store return values) |
# |--------------------------|------------------------------|------------------------------------------|
# | UK RITM- Loan Laptop           |searches for: Loan Laptop - Collection     | Loan Laptop - Collection |
# |                                |Loan Laptop - Drop Off     | Loan Laptop - Drop Off |
# |                                |Loan Laptop - Early Return     | Loan Laptop - Early Return |
# |                                |Loan Laptop - Return     | Loan Laptop - Return |
# |                                |Loan Laptop - Home Delivery     | Loan Laptop - Home Delivery |
# |                                |DTS Genesys Laptop    | DTS Genesys |
# |                                |anything other than above classification it reports as | Exception |

# In[44]:


### Classified Logic for a few Generic Request Activities
import re
def Std(dv_short_description, dv_workflow):
    if dv_workflow == "UK RITM- Loan Laptop":
        
        if 'Loan Laptop - Collection' in dv_short_description:
            return('Loan Laptop - Collection')
        
        elif 'Loan Laptop - Drop Off' in dv_short_description:
            return('Loan Laptop - Drop Off')
        
        elif 'Loan Laptop - Early Return' in dv_short_description:
            return('Loan Laptop - Early Return')
        
        elif 'Loan Laptop - Return' in dv_short_description:
            return('Loan Laptop - Return')
        
        elif 'Loan Laptop - Home Delivery' in dv_short_description:
            return('Loan Laptop - Home Delivery')
        
        elif 'DTS Genesys Laptop' in dv_short_description:
            return('DTS Genesys')
        
        else:
            return('Exception')
    else:
        pass    

df1['col0'] = df1.apply(lambda x: Std(x.dv_short_description,x.dv_workflow) if (x.dv_workflow.startswith("UK RITM- Loan Laptop")  & 
                                                              ("Complete" in x.dv_state)) else None, axis=1)                    

df1


# ### Generated Activity count for each tasks
# | dv_workflow              || col0 || Loan_Laptop_Collection (New col) |
# |--------------------------||------------------------------||-------|
# | UK RITM- Loan Laptop     || Loan Laptop - Collection|| 1       |
# 
# | dv_workflow              ||col0 || Loan_Laptop_Dropoff (New col)|
# |--------------------------||------------------------------||---|
# | UK RITM- Loan Laptop     || Loan Laptop - Drop Off || 1       |
# 
# | dv_workflow              || col0 ||  Loan_Laptop_Early_Return (New col) |
# |--------------------------||------------------------------||----|
# | UK RITM- Loan Laptop     || Loan Laptop - Early Return || 1     |
# 
# | dv_workflow              || col0 ||  Loan_Laptop_Return (New col)|
# |--------------------------||------------------------------||---|
# | UK RITM- Loan Laptop             || Loan Laptop - Return || 1     |
# 
# | dv_workflow              ||col0 ||   Loan_Laptop_Home_Delivery (New col) |
# |--------------------------||------------------------------||----|
# | UK RITM- Loan Laptop       || Loan Laptop - Home Delivery || 1   |
# 
# | dv_workflow              || col0 ||  Genesys_Laptop (New col) |
# |--------------------------||------------------------------||---|
# | UK RITM- Loan Laptop       || DTS Genesys || 1    |

# In[45]:


Loan_C = df1['dv_workflow'].str.startswith("UK RITM- Loan Laptop") & df1['col0'].str.startswith("Loan Laptop - Collection")
df1.loc[Loan_C, 'Loan_Laptop_Collection'] = str(1)

Loan_D = df1['dv_workflow'].str.startswith("UK RITM- Loan Laptop") & df1['col0'].str.startswith("Loan Laptop - Drop Off")
df1.loc[Loan_D, 'Loan_Laptop_Dropoff'] = str(1)

Loan_ER = df1['dv_workflow'].str.startswith("UK RITM- Loan Laptop") & df1['col0'].str.startswith("Loan Laptop - Early Return")
df1.loc[Loan_ER, 'Loan_Laptop_Early_Return'] = str(1)

Loan_R = df1['dv_workflow'].str.startswith("UK RITM- Loan Laptop") & df1['col0'].str.startswith("Loan Laptop - Return")
df1.loc[Loan_R, 'Loan_Laptop_Return'] = str(1)

Loan_HD = df1['dv_workflow'].str.startswith("UK RITM- Loan Laptop") & df1['col0'].str.startswith("Loan Laptop - Home Delivery")
df1.loc[Loan_HD, 'Loan_Laptop_Home_Delivery'] = str(1)

Loan_Gen = df1['dv_workflow'].str.startswith("UK RITM- Loan Laptop") & df1['col0'].str.startswith("DTS Genesys")
df1.loc[Loan_Gen, 'Genesys_Laptop'] = str(1)

df1


# ### Classification Logic for Standard Generic tasks (1:1)
# | dv_workflow                 ||  Classification_type (tmp col created to store return values) |
# |--------------------------||-------|
# | UK RITM Decom Request      ||  Decommission and Recovery of IT Hardware |
# | UK RITM - Equipment Move || Equipment Move || Equipment Move |

# In[46]:


df1.loc[(df1['dv_workflow'].astype(str).str.startswith("UK RITM Decom Request")), 'Classification_type']= 'Decommission and Recovery of IT Hardware'
                                                                                                                                      

df1.loc[(df1['dv_workflow'].astype(str).str.startswith("UK RITM - Equipment Move")) & \
       (df1['dv_short_description'].astype(str).str.startswith("Equipment Move")), 'Classification_type']= 'Equipment Move'


df1


# ### TVent Table
# | dv_workflow||dv_short_description||col (tmp col for storing return values)|
# |--------------------||--------------||-----|
# |UK DTS TVent Replenishment||searches for: Courier Delivery-Collection Administration UK||Courier Administration |
# |  ||CSG New Starter Asset||CSG New Starter Asset Assignment|
# |  ||CSG New Starter Call Off || CSG New Starter Call Off Admin|
# |  ||Stock Onsite Delivery || Stock Onsite Delivery |
# |  ||Worksetting Install Support || Worksetting Install Support|
# |  ||Deskside Setup Support || Worksetting Install Support |
# |  ||Courier Equipment Return || Courier Administration |
# |  ||Tech Vend Re-Stock || Tech Vend Re-Stock |
# |  ||Tech Sweep ||Tech Sweep |
# |  ||Rebuild || Hardware - Rebuild |
# |  ||Tech Bar ||Tech Bar Support |
# |  ||anything other than above classification it reports as || Exception |

# In[47]:


import re
def TVent(dv_short_description, dv_workflow):
    if dv_workflow == "UK DTS TVent Replenishment":
        
        #if 'Courier Delivery-Collection Administration' in dv_short_description:
            #return('Courier Administration')
        
        if 'Courier' in dv_short_description:
            return('Courier Administration')
        
        elif 'CSG New Starter Asset' in dv_short_description:
            return('CSG New Starter Asset Assignment')
        
        elif 'CSG New Starter Call Off' in dv_short_description:
            return('CSG New Starter Call Off Admin')
        
        elif 'Stock Onsite Delivery' in dv_short_description:
            return('Stock Onsite Delivery')
        
        elif 'Worksetting Install Support' in dv_short_description:
            return('Worksetting Install Support')
        
        elif 'Deskside Setup Support' in dv_short_description:
            return('Worksetting Install Support')
        
        #elif 'Courier Equipment Return' in dv_short_description:
            #return('Courier Administration')
        
        elif 'Tech Vend Re-Stock' in dv_short_description:
            return('Tech Vend Re-Stock')
        
        elif 'Tech Sweep' in dv_short_description:
            return('Tech Sweep')
        
        elif 'Rebuild' in dv_short_description:
            return('Hardware - Rebuild')
        
        elif 'Tech Bar' in dv_short_description:
            return('Tech Bar Support')
        
        else:
            return('Exception')
    
    else:
        pass    

df1['col'] = df1.apply(lambda x: TVent(x.dv_short_description,x.dv_workflow) if (x.dv_workflow.startswith("UK DTS TVent Replenishment")  & 
                                                              ("Complete" in x.dv_state)) else None, axis=1)                    

df1


# | dv_workflow||dv_short_description||col1 (tmp col for storing return values)|
# |--------------------||--------------||-----|
# |COVID - Equipment Return || searches for: Leaver || Leaver DTS Record|
# |                         || if Leaver does not exist || Courier Administration|

# In[48]:


# Generic Request
import re
def covid(dv_short_description, dv_workflow):
    if dv_workflow == "COVID - Equipment Return":
        if 'Leaver' in dv_short_description:
            return('Leaver DTS Record')
        else:
            return('Courier Administration')
    else:
        pass    

df1['col1'] = df1.apply(lambda x: covid(x.dv_short_description,x.dv_workflow) if (x.dv_workflow.startswith("COVID - Equipment Return")  & 
                                                              ("Complete" in x.dv_state)) else None, axis=1)                    

df1


# | dv_workflow||COVID - Equipment Return(tmp col for storing return values)|
# |--------------------||--------------|
# |COVID - Equipment Return|| 1  |

# In[49]:


Covid_Leaver_Admin = df1['dv_workflow'].str.startswith("COVID - Equipment Return")
df1.loc[Covid_Leaver_Admin, 'COVID - Equipment Return'] = str(1)



# | dv_workflow||Classification_type|
# |--------------------||--------------|
# |RITM - Sky work setting Request|| Worksetting Install Support  |

# In[50]:


#New change
df1.loc[(df1['dv_workflow'].astype(str).str.startswith("RITM - Sky work setting Request")),'Classification_type']= 'Worksetting Install Support'
                                                                                                                      


# | dv_workflow||work_setting(tmp col for storing return values)|
# |--------------------||--------------|
# |RITM - Sky work setting Request|| 1  |

# In[51]:


Sky = df1['dv_workflow'].str.startswith("RITM - Sky work setting Request")
df1.loc[Sky, 'work_setting'] = str(1)


# ### Calculating the Activity Count for all TVent Request
# |dv_workflow ||description || Activity Count |
# |------------||-----------||-----------------------------------|
# |UK DTS TVent Replenishment||search for Time Band in description || Time Band - number / 30 |
# 

# In[52]:


# Calculating Time Band for Generic Request Activity
# Defining a function
def func(text,str_to_find='Time Band - '):
    # find the position
    position=text.find('Time Band - ')
    if(position<0):
         # string not find, return -1 
        return -1
    # get the position of the integer next to the string
    start=position+len(str_to_find)
    # get the string with the integer
    nr_string=text[start:].lstrip().split(' ',1)[0]
    # seperate integer from string
    number=int(nr_string)
    
    return int(np.ceil(number/30))


# In[53]:


# Calling the above func() on each request

#df1['Courier_Delivery_Collection_Administration'] = df1.apply(lambda x: func(x.description) if (x.dv_workflow.startswith("UK DTS TVent Replenishment") &
                                                                                                   #("Courier Delivery-Collection Administration" in x.dv_short_description)) else None, axis=1)

df1['Courier_Administration'] = df1.apply(lambda x: func(x.description) if (x.dv_workflow.startswith("UK DTS TVent Replenishment") &
                                                                                                   ("Courier" in x.dv_short_description)) else None, axis=1)

df1['CSG_New_Starter_Asset'] = df1.apply(lambda x: func(x.description) if (x.dv_workflow.startswith("UK DTS TVent Replenishment") & 
                                                                                                    ("CSG New Starter Asset" in x.dv_short_description)) else None, axis=1)

df1['CSG_New_Starter_Call_Off'] = df1.apply(lambda x: func(x.description) if (x.dv_workflow.startswith("UK DTS TVent Replenishment") & 
                                                                                                     ("CSG New Starter Call Off" in  x.dv_short_description)) else None, axis=1)

df1['Stock_Onsite_Delivery'] = df1.apply(lambda x: func(x.description) if (x.dv_workflow.startswith("UK DTS TVent Replenishment") & 
                                                                         ("Stock Onsite Delivery" in x.dv_short_description)) else None, axis=1)

df1['Worksetting_Install_Support_1'] = df1.apply(lambda x: func(x.description) if (x.dv_workflow.startswith("UK DTS TVent Replenishment") & 
                                                                               ("Worksetting Install Support" in x.dv_short_description)) else None, axis=1)

df1['Worksetting_Install_Support_2'] = df1.apply(lambda x: func(x.description) if (x.dv_workflow.startswith("UK DTS TVent Replenishment") & 
                                                                            ("COVID Deskside Setup Support" in x.dv_short_description)) else None, axis=1)


#df1['Desktype_Setup_Support'] = df1.apply(lambda x: func(x.description) if (x.dv_workflow.startswith("UK DTS TVent Replenishment") & 
                                                                            #("COVID Deskside Setup Support" in x.dv_short_description)) else None, axis=1)

#df1['Courier_Administration'] = df1.apply(lambda x: func(x.description) if (x.dv_workflow.startswith("UK DTS TVent Replenishment") & 
                                                                              #("Courier Equipment Return" in x.dv_short_description)) else None, axis=1)

df1['Tech_Vend_ReStock'] = df1.apply(lambda x: func(x.description) if (x.dv_workflow.startswith("UK DTS TVent Replenishment") & 
                                                                      ("Tech Vend Re-Stock" in x.dv_short_description)) else None, axis=1)

df1['Tech_Sweep'] = df1.apply(lambda x: func(x.description) if (x.dv_workflow.startswith("UK DTS TVent Replenishment") & 
                                                              ("Tech Sweep" in x.dv_short_description)) else None, axis=1)

df1['Rebuild'] = df1.apply(lambda x: func(x.description) if (x.dv_workflow.startswith("UK DTS TVent Replenishment") & 
                                                              ("Rebuild" in x.dv_short_description)) else None, axis=1)

df1['Tech_Bar'] = df1.apply(lambda x: func(x.description) if (x.dv_workflow.startswith("UK DTS TVent Replenishment") & 
                                                              ("Tech Bar" in x.dv_short_description)) else None, axis=1)

df1


# ### Calculating the Activity Count for all TVent Request
# |dv_workflow ||description |
# |------------||-----------|
# |UK RITM Decom Request||search for: Quantity of Monitors || Formula for calculation |
# |                     ||            Quantity of Laptops  || Monitors Only = Quantity of Monitors - (Quantity of Laptops + Quantity of Desktops/Workstations |
# |                     ||            Quantity of Desktops/Workstations  |
# |                     ||            Quantity of Printers  |
# |                     ||            Quantity of Laptops  |  
# 
# #### Formula used for Calculation of Decommission and Recovery of IT Hardware
#  $ Monitors Only $ = Quantity of Monitors - (Quantity of Laptops + Quantity of Desktops/Workstations)
#  
#  $ Activity Count $ = (($ Monitors Only $ + Quantity of Laptops + Quantity of Desktops/Workstations)/5) + Quantity of Printers 
#     
# 
# 

# In[54]:
### Logic for Decommission and recovery of IT hardware
import re 
import math

def count(text, nr):
    reg = re.findall(r'Quantity of [A-Za-z\/]+\:\s+\S+\s+\d+',text)
    dict_items={}
    for each in reg:
        each = each.split(':')
        item_name = each[0][11:].strip()
        Quantity = int(each[1].split('-')[1].strip())
        dict_items[item_name]=Quantity
        #print(dict_items)
        #print(nr, " ",dict_items)
    #if('Monitors' in dict_items):
        #quant_monitors=dict_items['Monitors']
    #else:
        #quant_monitors=0
    
    if 'Monitors' in dict_items:
        quant_monitors = dict_items['Monitors']
    
    else:
        quant_monitors = 0
        
    if 'Monitors/TVs' in dict_items:
        quant_monitorstvs = dict_items['Monitors/TVs']
    else:
        quant_monitorstvs = 0






    
    #if('Monitors' in dict_items):
        #quant_monitors = dict_items['Monitors']
    #elif('Monitors/TVs' in dict_items):
        #quant_monitorstvs=dict_items['Monitors/TVs'] 
    #else:
        #quant_monitors=0
            
        
        
    #if('Monitors/TVs' in dict_items):
        #quant_monitorstvs=dict_items['Monitors/TVs']
    #else:
        #quant_monitorstvs=0 
    
    if('Laptops' in dict_items):
        quant_laptops=dict_items['Laptops']
    else:
        quant_laptops=0

    if('Desktops/Workstations' in dict_items):
        quant_desktops_or_workstations=dict_items['Desktops/Workstations']
    else:
        quant_desktops_or_workstations=0
        
    if('Printers' in dict_items):
        quant_printer=dict_items['Printers']
    else:
        quant_printer=0

    Monitors_only=(quant_monitors + quant_monitorstvs)  - (  quant_desktops_or_workstations)
    
    if Monitors_only < 0:
        Monitors_only = 0 
    Activity_count = math.ceil((Monitors_only + quant_desktops_or_workstations)/5) + quant_printer 
    
    return Activity_count
    

df1['decom_count'] = df1.apply(lambda x: count(x.description, x.number) if (x.dv_workflow.startswith("UK RITM Decom Request")) else None, axis=1)

#checkdf1['decom_count'] = df1.apply(lambda x: count(x.description, x.number) if (x.dv_workflow.startswith("UK RITM Decom Request") & 
                                                             #("Decommission and Recovery of IT Hardware" in x.dv_short_description )) else None, axis=1)


df1





# In[55]:


#### Decommission of IT Hardware, for other equipments 
df1=df1.replace(to_replace={'decom_count': {0: 1}})
df1.to_excel('fileoutcc.xlsx')


# ### Calculating the Activity Count for Equipment Move
# |dv_workflow ||description |
# |------------||-----------|
# |UK RITM - Equipment Move||search for: Quantity of Monitors || Formula for calculation |
# |                     ||            Quantity of Laptops  || Monitors Only = Quantity of Monitors - (Quantity of Laptops + Quantity of Desktops/Workstations |
# |                     ||            Quantity of Desktops/Workstations  |
# |                     ||            Quantity of Printers  |
# |                     ||            Quantity of Laptops  |  
# 
# #### Formula used for Calculation of Equipment Move
#  $ MoveCountHardware $ = (Quantity of Laptops + Quantity of Desktops/Workstations + Quantity of Monitors) / 2 + Quantity of Printers
#  
# 

# In[56]:


#### Logic for Equipment Move
import re
import math

def equip_move_count(text, nr):
    

    reg = re.findall(r'Quantity of [A-Za-z \/]+\:\s+\S+\s+\d+',text)
    dict_items={}
    for each in reg:
        each = each.split(':')
        item_name = each[0][11:].strip()
        Quantity = int(each[1].split('-')[1].strip())
        dict_items[item_name]=Quantity
        #print(nr, " ",dict_items)
    if('Monitors' in dict_items):
        quant_monitor=dict_items['Monitors']
    else:
        quant_monitor=0

    #if 'Monitors' in dict_items or 'Monitors/TVs' in dict_items:
        #quant_monitors = dict_items.get('Monitors', 0) + dict_items.get('Monitors/TVs', 0)
    #else:
        #quant_monitors = 0    
        
    if('Laptops' in dict_items):
        quant_laptops=dict_items['Laptops']
    else:
        quant_laptops=0

    if('Desktops/Workstation' in dict_items):
        quant_desktop_or_Workstation=dict_items['Desktops/Workstation']
    else:
        quant_desktop_or_Workstation=0
    #print()
    #print(dict_items)
    move_count_hardware = math.ceil((dict_items['Laptops'] + dict_items['Desktops/Workstations'] + dict_items['Monitors'])/2) + dict_items['Printers']
#a=print(move_count_hardware)
    return move_count_hardware
#return (str(move_count_hardware) + b)

df1['equi_move_count'] = df1.apply(lambda x: equip_move_count(x.description, x.number) if (x.dv_workflow.startswith("UK RITM - Equipment Move") & 
                                                             x.dv_short_description.startswith("Equipment Move")) else None, axis=1)
df1


# In[57]:


df1.dtypes


# In[58]:


#df1.to_excel('fileout.xlsx')


# In[59]:


df1 = df1.replace({pd.NaT: '', np.NaN: ''})
df1


# In[60]:


df1.dtypes


# In[61]:


### Combining the data from tmp created columns to get the final column named as 'Auto_Classification_type'
df1['Auto_Classification_type'] = df1['Classification_type']+ df1['col'] + df1['col0'] + df1['col1']  


# In[62]:


### To bring all the columns to a single datatype
df1['Courier_Administration'] = df1['Courier_Administration'].astype(str)
df1['CSG_New_Starter_Asset'] = df1['CSG_New_Starter_Asset'].astype(str)
df1['CSG_New_Starter_Call_Off'] = df1['CSG_New_Starter_Call_Off'].astype(str)
df1['Stock_Onsite_Delivery'] = df1['Stock_Onsite_Delivery'].astype(str)
df1['Worksetting_Install_Support_1'] = df1['Worksetting_Install_Support_1'].astype(str)
df1['Worksetting_Install_Support_2'] = df1['Worksetting_Install_Support_2'].astype(str)
#df1['Desktype_Setup_Support'] = df1['Desktype_Setup_Support'].astype(str)
#df1['Courier_Administration'] = df1['Courier_Administration'].astype(str)
#df1['Courier_Equipment_Return'] = df1['Courier_Equipment_Return'].astype(str)
df1['Tech_Vend_ReStock'] = df1['Tech_Vend_ReStock'].astype(str)
df1['Tech_Sweep'] = df1['Tech_Sweep'].astype(str)
df1['Tech_Bar'] = df1['Tech_Bar'].astype(str)
df1['Rebuild'] = df1['Rebuild'].astype(str)
df1['decom_count'] = df1['decom_count'].astype(str)
df1['equi_move_count'] = df1['equi_move_count'].astype(str)
df1['quantity'] = df1['quantity'].astype(str)
#df1['closed_date'] = df1['closed_date'].astype(str)
df1.dtypes


# In[63]:


### Combining the data from tmp created columns where Activity count was collected for generic requests to get output in the final column named as 'Activity_count'
df1['Activity_count'] = df1['Accessibility_Request'] + df1['UK RITM - DTS Leaver'] + df1['Loan_Laptop_Collection'] + df1['Loan_Laptop_Dropoff'] + df1['Loan_Laptop_Early_Return'] + df1['Loan_Laptop_Return'] + df1['Loan_Laptop_Home_Delivery'] + df1['Genesys_Laptop']+ df1['COVID - Equipment Return'] + df1['work_setting'] + df1['Software_Bundle'] + df1['Courier_Administration'] +df1['CSG_New_Starter_Asset'] + df1['CSG_New_Starter_Call_Off'] + df1['Stock_Onsite_Delivery'] + df1['Worksetting_Install_Support_1'] + df1['Worksetting_Install_Support_2'] + df1['Tech_Vend_ReStock'] + df1['Tech_Sweep'] + df1['Rebuild'] + df1['Tech_Bar'] + df1['decom_count'] + df1['equi_move_count']
df1


# In[64]:


### Performing check to confirm whether there exist any Null values in Auto_Classification_type
df1=df1.replace(to_replace={'Auto_Classification_type': {'': 'Exception'}})


# In[65]:


df1


# In[66]:


### Check for Activity count to see any Null values and report as Exception
df1=df1.replace(to_replace={'Activity_count': {'': 'Exception'} })


# In[67]:


df1


# In[68]:


#df1.to_excel('fileout.xlsx')


# # Truncated Data Frame
# ##### Containing data for Tech-Store orders

# In[69]:


df2


# In[70]:


df2['closed_date'] = pd.DatetimeIndex(df2['closed_at']).date


# In[71]:


### Handling Case sensitvity in the Data
df2['dv_u_model_type']=df2['dv_u_model_type'].str.upper()
df2


# |dv_workflow||dv_u_model_type |
# |------------||-----------|
# |Tech-Store - Standard Hardware Item||searches for: LAPTOP|
# |                                   || MONITOR|
# |                                   || DESKTOP|
# |                                   || TABLET|
# |                                   || ACCESSORY|
# |                                   || DOCKING STATION|
# |                                   || WORKSTATION|
# |                                   || PRINTER|
# |                                   || any new addition gets reported as Exception|

# In[72]:


import re
def model(dv_u_model_type, dv_workflow):
    if dv_workflow == "Tech-Store - Standard Hardware Item":
        if 'LAPTOP' in dv_u_model_type:
            return('Laptop')
        elif 'MONITOR' in dv_u_model_type:
            return('Monitor')
        elif 'DESKTOP' in dv_u_model_type:
            return('Desktop')
        elif 'TABLET' in dv_u_model_type:
            return('Accessory')
        elif 'ACCESSORY' in dv_u_model_type:
            return('Accessory')
        elif 'DOCKING STATION' in dv_u_model_type:
            return('Docking_Station')
        elif 'WORKSTATION' in dv_u_model_type:
            return('Work_Station')
        elif 'PRINTER' in dv_u_model_type:
            return('Printer')
        else:
            return('Exception')
    else:
        pass    

df2['dv_u_model_type'] = df2.apply(lambda x: model(x.dv_u_model_type,x.dv_workflow) if (x.dv_workflow.startswith("Tech-Store - Standard Hardware Item")  & 
                                                              ("Complete" in x.dv_state)) else None, axis=1)                    

df2


# In[73]:


df2.dtypes


# In[74]:


### For New Starters Only the first delivery is assigned under New Starters, rest of the tasks which related to the same REQ are calculated in regular bundle Calculations. 
# Filter data where `catitem` is 'Tech-Store - Standard Hardware Item'
#df2 = df2[df['dv_workflow'] == 'Tech-Store - Standard Hardware Item']

# Create a new column `new_starter` with default value as 'No'
df2['new_starter'] = 'No'

# Get unique REQ numbers
unique_numbers = df2['dv_request'].unique()

# Loop over unique REQ numbers
for number in unique_numbers:
    # Filter rows corresponding to unique REQ number
    temp_df = df2[df2['dv_request'] == number]
    # Get unique dates for current REQ number
    unique_dates = temp_df['closed_date'].unique()
    # Loop over unique dates
    for date in unique_dates:
        # Filter rows corresponding to unique date
        temp_df_2 = temp_df[temp_df['closed_date'] == date]
        # Check if there is any row containing 'New Starter' in `dv_short_description` column
        if any(temp_df_2['dv_short_description'].str.contains('New Starter')):
            # If yes, update `new_starter` column with 'Yes' for all rows with same date
            df2.loc[(df2['dv_request'] == number) & (df2['closed_date'] == date), 'new_starter'] = 'Yes'

df2


# In[75]:


df2['dv_short_description'] = df2.apply(lambda row: "New Starter" + row['dv_short_description'] if "Yes" in row['new_starter'] else row['dv_short_description'], axis=1)
df2


# In[76]:


### Adding suffix NS before model type to differentiate if the device belongs to New Starter.
#df2['dv_short_description'] = df2.apply(lambda row: "New Starter " + row['dv_short_description'] if "New Starter" in row['dv_short_description'] and "Tech-Store" in row['dv_workflow'] else row['dv_short_description'], axis=1)
df2['dv_u_model_type'] = df2.apply(lambda row: "NS_" + row['dv_u_model_type'] if "New Starter" in row['dv_short_description'] else row['dv_u_model_type'], axis=1)

#print(df)

df2


# In[77]:


### to differentiate if the device belongs to New Starter request or a regular request
df3=df2.replace(to_replace={'dv_u_model_type': {'NS_Accessory': 'NS_Acc', 'NS_Work_Station': 'NS_WS', 'NS_Desktop': 'NS_DT', 'NS_Laptop': 'NS_Lap', 
                                                'NS_Docking_Station': 'NS_DS', 'NS_Monitor': 'NS_Mon'}})
df3


# In[78]:


df3['quantity'] = df3['quantity'].astype(str)
df3.dtypes


# In[79]:


df3['Model_quantity_count'] = df3['quantity'] + df3['dv_u_model_type']
df3


# In[80]:


df3['Model_type']=df3.apply(lambda x: str(x['dv_u_model_type'])*int(x['quantity']) if(str(x['quantity']).strip().isdigit()) else 'Exception',axis=1)
df3


# In[81]:


### Grouping the data based on unique REQNo and its corresponding closed date(with same dates grouped togeather)
df4 = df3.groupby(['dv_request', 'closed_date'], as_index=False)[['dv_request_item','number','Model_type', 'Model_quantity_count', 'dv_workflow', 'dv_state']].sum()
df4


# In[ ]:





# In[82]:


## Bundles Logic including new starter
import math
# hardware bundle New Starter -> 1 hardware  + upto 3 peripherals ( 1 docking station + 1 or 2 monitors ) + upto 7 accessories
# hardware bundle -> 1 hardware + upto 3 peripherals ( 1 docking station + 1 or 2 monitors ) + upto 7 accessories
# consumable bundle -> upto 20 accessories
# peripheral bundle -> 1 docking station + 1 or 2 monitors + upto 6 accessories
# printer -> 1 printer
def bundle_calculator(printer_count,ns_laptop_count,ns_desktop_count,ns_workstation_count,ns_docking_station_count,ns_monitor_count,ns_accessories_count,laptop_count,desktop_count,workstation_count,docking_station_count,monitor_count,accessories_count):
    bundle_dict={}
    # initialise all the bundles to zero
    #hardware_bundle,ns_hardware_bundle,consumable_bundle,peripheral_bundle,peripheral_printer=0,0,0,0,0
    peripheral_printer,ns_hardware_bundle,ns_consumable_bundle,ns_peripheral_bundle,hardware_bundle,consumable_bundle,peripheral_bundle=0,0,0,0,0,0,0
    
    # printer -> 1 printer
    if printer_count > 0 :
        peripheral_printer = printer_count 
        bundle_dict['Printer'] = peripheral_printer
    else:
        peripheral_printer = 0
        bundle_dict['Printer']=peripheral_printer    
   
    # hardware bundle New Starter -> 1 hardware  + upto 3 peripherals ( 1 docking station + 1 or 2 monitors ) + upto 7 accessories    
    if ns_laptop_count > 0 or ns_desktop_count > 0 or ns_workstation_count > 0:
        ns_hardware_bundle = ns_laptop_count + ns_desktop_count + ns_workstation_count
        bundle_dict['NS_hardware_bundle'] = ns_hardware_bundle
        ns_monitors = math.ceil(ns_monitor_count/2)
        ns_peripheral_bundle = -ns_hardware_bundle
        ns_consumable_bundle = -7*ns_hardware_bundle
        ns_maxperipherals = max(ns_docking_station_count,ns_monitors)
        ns_peripheral_bundle = ns_peripheral_bundle + ns_maxperipherals
        if ns_peripheral_bundle > 0 :
            bundle_dict['NS_peripheral_bundle'] = ns_peripheral_bundle
            if ns_accessories_count + ns_consumable_bundle > 0 :
                ns_temp = -6*ns_peripheral_bundle
                ns_consumable_bundle = ns_temp + ns_consumable_bundle
                if ns_consumable_bundle + ns_accessories_count > 0:
                    ns_consumable_bundle = ns_consumable_bundle + ns_accessories_count
                    ns_consumable_bundle = math.ceil(ns_consumable_bundle/20)
                else:
                    ns_consumable_bundle = 0
                bundle_dict['NS_consumable_bundle']=ns_consumable_bundle
            else:
                ns_consumable_bundle = 0
                bundle_dict['NS_consumable_bundle']=ns_consumable_bundle
        else:
            ns_peripheral_bundle = 0
            bundle_dict['NS_peripheral_bundle'] = ns_peripheral_bundle
            if ns_accessories_count + ns_consumable_bundle > 0:
                ns_consumable_bundle = math.ceil((ns_accessories_count + ns_consumable_bundle)/20)
            else:
                ns_consumable_bundle = 0
            bundle_dict['NS_consumable_bundle']=ns_consumable_bundle
     
    else:
        bundle_dict['NS_hardware_bundle'] = ns_hardware_bundle
        ns_monitors = math.ceil(ns_monitor_count/2)
        ns_maxperipherals = max(ns_docking_station_count,ns_monitors)
        ns_peripheral_bundle = ns_peripheral_bundle + ns_maxperipherals
        if ns_peripheral_bundle > 0 :
            bundle_dict['NS_peripheral_bundle'] = ns_peripheral_bundle
            if ns_accessories_count - (6*ns_peripheral_bundle) > 0:
                ns_consumable_bundle = math.ceil((ns_accessories_count - (6*ns_peripheral_bundle))/20)
            else:
                ns_consumable_bundle = 0
            bundle_dict['NS_consumable_bundle']=ns_consumable_bundle
        else:
            ns_peripheral_bundle = 0
            bundle_dict['NS_peripheral_bundle'] = ns_peripheral_bundle
            if ns_accessories_count > 0:
                ns_consumable_bundle = math.ceil(ns_accessories_count/20)
            else:
                ns_consumable_bundle = 0
            bundle_dict['NS_consumable_bundle']=ns_consumable_bundle 
    # hardware bundle -> 1 hardware + upto 3 peripherals ( 1 docking station + 1 or 2 monitors ) + upto 7 accessories
    # consumable bundle -> upto 20 accessories
    # peripheral bundle -> 1 docking station + 1 or 2 monitors + upto 6 accessories
    if laptop_count > 0 or desktop_count > 0 or workstation_count > 0:
        hardware_bundle = laptop_count + desktop_count + workstation_count
        bundle_dict['hardware_bundle'] = hardware_bundle
        monitors = math.ceil(monitor_count/2)
        peripheral_bundle = -hardware_bundle
        consumable_bundle = -7*hardware_bundle
        maxperipherals = max(docking_station_count,monitors)
        peripheral_bundle = peripheral_bundle + maxperipherals
        if peripheral_bundle > 0 :
            bundle_dict['peripheral_bundle'] = peripheral_bundle
            if accessories_count + consumable_bundle > 0 :
                temp = -6*peripheral_bundle
                consumable_bundle = temp + consumable_bundle
                if consumable_bundle + accessories_count > 0:
                    consumable_bundle = consumable_bundle + accessories_count
                    consumable_bundle = math.ceil(consumable_bundle/20)
                else:
                    consumable_bundle = 0
                bundle_dict['consumable_bundle']=consumable_bundle
            else:
                consumable_bundle = 0
                bundle_dict['consumable_bundle']=consumable_bundle
        else:
            peripheral_bundle = 0
            bundle_dict['peripheral_bundle'] = peripheral_bundle
            if accessories_count + consumable_bundle > 0:
                consumable_bundle = math.ceil((accessories_count + consumable_bundle)/20)
            else:
                consumable_bundle = 0
            bundle_dict['consumable_bundle']=consumable_bundle
    else:
        bundle_dict['hardware_bundle'] = hardware_bundle
        monitors = math.ceil(monitor_count/2)
        maxperipherals = max(docking_station_count,monitors)
        peripheral_bundle = peripheral_bundle + maxperipherals
        if peripheral_bundle > 0 :
            bundle_dict['peripheral_bundle'] = peripheral_bundle
            if accessories_count - (6*peripheral_bundle) > 0:
                consumable_bundle = math.ceil((accessories_count - (6*peripheral_bundle))/20)
            else:
                consumable_bundle = 0
            bundle_dict['consumable_bundle']=consumable_bundle
        else:
            peripheral_bundle = 0
            bundle_dict['peripheral_bundle'] = peripheral_bundle
            if accessories_count > 0:
                consumable_bundle = math.ceil(accessories_count/20)
            else:
                consumable_bundle = 0
            bundle_dict['consumable_bundle']=consumable_bundle
       
    bundleString=''
    for k,v in bundle_dict.items():
        bundleString=bundleString+str(k)+' : '
        bundleString=bundleString+str(v)+' '
   
    return bundleString



def extracter(item, itemslist):
    M = len(item)
    N = len(itemslist)
    res = 0
    for i in range(N - M + 1):
        j = 0
        while j < M:
            if (itemslist[i + j] != item[j]):
                break
            j += 1
        if (j == M):
            res += 1
            j = 0
    return res

def extract_product(items_list):
    #extract the count of all items in the request
    #printer_count = extracter('Printer',items_list)
    ns_laptop_count,ns_desktop_count,ns_workstation_count,ns_docking_station_count,ns_monitor_count,ns_accessories_count,printer_count,laptop_count,desktop_count,workstation_count,docking_station_count,monitor_count,accessories_count=0,0,0,0,0,0,0,0,0,0,0,0,0
    printer_count = extracter('Printer',items_list)
    ns_laptop_count = extracter('NS_Lap',items_list)
    ns_desktop_count = extracter('NS_DT',items_list)
    ns_workstation_count = extracter('NS_WS',items_list)
    ns_docking_station_count = extracter('NS_DS',items_list)
    ns_monitor_count = extracter('NS_Mon',items_list)
    ns_accessories_count = extracter('NS_Acc',items_list)
    laptop_count = extracter('Laptop',items_list)
    desktop_count = extracter('Desktop',items_list)
    workstation_count = extracter('Work_Station',items_list)
    docking_station_count = extracter('Docking_Station',items_list)
    monitor_count = extracter('Monitor',items_list)
    accessories_count = extracter('Accessory',items_list)
    #printer_count = extracter('Printer',items_list)
    #ns_printer_count = extracter('NS_Printer',items_list)
    bundles_count = bundle_calculator(printer_count,ns_laptop_count,ns_desktop_count,ns_workstation_count,ns_docking_station_count,ns_monitor_count,ns_accessories_count,laptop_count,desktop_count,workstation_count,docking_station_count,monitor_count,accessories_count)
    return bundles_count
def bun(text):
    bundles = extract_product(text)
# bundles output is a dict that is added as new column in the df
    return(bundles)

#item = 'NS_LapNS_AccAccessoryAccessoryAccessoryAccessoryDocking_Station'
#bundles = extract_product(item)
# item should be sent as input to the function extract_product(item)

#print(bundles)


# In[83]:


# data frame containing the count of bundles
df4['Bundles_count'] = df4.apply(lambda x: bun(x.Model_type) if (x.dv_workflow.startswith("Tech-Store - Standard Hardware Item") & 
                                                                ("Complete" in x.dv_state)) else None, axis=1)


df4


# In[84]:


import pandas as pd
import re


dflist = df4.values.tolist()

def bundle_extractor(bundles):
    bundles = bundles.split()
    bundles_dict={}
    bundles_dict[bundles[0]]=bundles[2]
    bundles_dict[bundles[3]]=bundles[5]
    bundles_dict[bundles[6]]=bundles[8]
    bundles_dict[bundles[9]]=bundles[11]
    bundles_dict[bundles[12]]=bundles[14]
    bundles_dict[bundles[15]]=bundles[17]
    bundles_dict[bundles[18]]=bundles[20]
    
    return bundles_dict


#split the ritm and sctask
for each in dflist:
    ritmstr = each[2]
    taskstr = each[3]
    regexritm = '(RITM\d+)'
    regextask = '(SCTASK\d+)'
    ritmlist = re.findall(regexritm,ritmstr)
    tasklist = re.findall(regextask,taskstr)
    each[2] = ritmlist
    each[3] = tasklist

fdf=[]

for each in dflist:
    bundledict = bundle_extractor(each[-1])
    activity_to_append=[]
    activity_count=[]
    row=[]
    request=each[0]
    date=each[1]
    bundles_overall=each[-1]
    for k,v in bundledict.items():
        if int(v) > 0:
            activity_to_append.append(k)
            activity_count.append(v)
    if len(each[2]) > 1:
        #muliple ritm and tasks present
        if len(activity_to_append)>1:
            n = len(each[2])
            for k in range(n):
                temp_row=[]
                temp_row.append(request)
                temp_row.append(date)
                temp_row.append(each[2][k])
                temp_row.append(each[3][k])
                if k == 0:
                    temp_row.append(bundles_overall)
                    temp_row.append(activity_to_append[k])
                    temp_row.append(activity_count[k])
                elif k < len(activity_to_append):
                    temp_row.append('')
                    temp_row.append(activity_to_append[k])
                    temp_row.append(activity_count[k])
                else:
                    temp_row.append('')
                    temp_row.append('')
                    temp_row.append('')
                row.append(temp_row)
        else:
            n = len(each[2])
            for k in range(n):
                temp_row=[]
                temp_row.append(request)
                temp_row.append(date)
                temp_row.append(each[2][k])
                temp_row.append(each[3][k])
                if k==0:
                    temp_row.append(bundles_overall)
                    temp_row.append(activity_to_append[0])
                    temp_row.append(activity_count[0])
                else:
                    temp_row.append('')
                    temp_row.append('')
                    temp_row.append('')
                row.append(temp_row)
    else:
        #single ritm and task
        if len(activity_to_append)>1:
            m = len(activity_to_append)
            for i in range(m):
                temp_row=[]
                temp_row.append(request)
                temp_row.append(date)
                temp_row.append(each[2][0])
                temp_row.append(each[3][0])
                if i == 0:
                    temp_row.append(bundles_overall)
                else:
                    temp_row.append('')
                temp_row.append(activity_to_append[i])
                temp_row.append(activity_count[i])
                row.append(temp_row)
        else:
            row.append(request)
            row.append(date)
            row.append(each[2][0])
            row.append(each[3][0])
            row.append(bundles_overall)
            row.append(activity_to_append[0])
            row.append(activity_count[0])
    if(isinstance(row[0], list)):
        for eachrow in row:
            fdf.append(eachrow)
    else:
        fdf.append(row)

df5=pd.DataFrame(fdf,columns=['dv_request','closed_date','dv_request_item','number','bundles_count', 'Auto_Classification_type','Activity_count'])

df5.to_excel('fileout.xlsx')


# In[85]:


df5


# In[86]:


# Replace all empty string with duplicates in the Auto_Classification_type column
# df5['Auto_Classification_type'] = df5['Auto_Classification_type'].replace('', 'Duplicate')


# In[87]:


#df5.to_excel('fileout1.xlsx')


# In[ ]:





# In[88]:


# concatenating df1 and df4 along rows
df6 = vertical_concat = pd.concat([df1, df5], axis=0, ignore_index = True)
df6 


# In[89]:


#### Now I have df which contains complete data, next stp would be select those col which are common in both and the aditional col to be added from main data frame


# In[90]:


columns_subset_extract_2=['number', 'dv_request', 'dv_request_item', 'closed_date', 'Auto_Classification_type', 
                          'Activity_count'] 
                        
df6 = df6[columns_subset_extract_2]
df6
#df_data_extract=df_data_extract[columns_subset_extract]

#merged_df = pd.merge(df5, df_data_extract, on=['number', 'dv_request', 'dv_request_item', 'closed_date', 'key5'])
#merged_df = pd.merge(df5, df_data_extract, on='number')
auto_sctask_df = pd.merge(df6, df, on='number')
#auto_sctask_df = pd.merge(df_data_extract_leavers, pd.merge(df6, df, on='number'), on='number')
auto_sctask_df


# In[107]:


column_subset_extract3=['number', 'dv_request_x', 'dv_request_item_x', 'closed_date_x', 'closed_at', 'dv_assignment_group', 'user_location', 'dv_u_model_type', 'dv_workflow', 'quantity', 'dv_short_description', 'description', 'dv_u_completion_method', 'Auto_Classification_type',
                         'dv_u_completion_category', 'dv_state', 'Activity_count', 'Business_group', 'assign_classification', 'yearmonth']
#merged_df=merged_df[column_subset_extract3]
auto_sctask_df=auto_sctask_df[column_subset_extract3]


# In[108]:


### After merge few column names changed, here we bring them back to original spark extract(names)
column_names_merge_extract=['number', 'dv_request_x', 'dv_request_item_x', 'closed_date_x', 'closed_at', 'dv_assignment_group', 'user_location', 'dv_u_model_type', 'dv_workflow', 'quantity', 'dv_short_description', 'description',
                         'dv_u_completion_method','Auto_Classification_type', 'dv_u_completion_category', 'dv_state', 'Activity_count', 'Business_group', 'assign_classification', 'yearmonth']
column_names_original=['number', 'dv_request', 'dv_request_item', 'closed_date', 'closed_at', 'dv_assignment_group', 'user_location', 'dv_u_model_type', 'dv_workflow', 'quantity', 'dv_short_description', 'description', 
                       'Completion_method', 'Auto_Classification_type', 'dv_u_completion_category', 'dv_state', 'Activity_count', 'Business_group', 'assign_classification', 'yearmonth']

for i in range(len(column_names_merge_extract)):
    #merged_df=merged_df.rename(columns={column_names_merge_extract[i]:column_names_original[i]})
    auto_sctask_df=auto_sctask_df.rename(columns={column_names_merge_extract[i]:column_names_original[i]})


# In[109]:


auto_sctask_df['closed_date'] = pd.DatetimeIndex(auto_sctask_df['closed_at']).date


# In[110]:


auto_sctask_df

# In[111]:naya2


### add output to check
#auto_sctask_df.to_excel('fileout_Exceptions.xlsx')


# In[112]:


import re

def DTS(dv_short_description, dv_workflow, current_classification):
    if dv_workflow == "Tech-Store - Standard Hardware Item":
        if 'DTS Genesys' in dv_short_description:
            return('DTS Genesys')
        else:
            return current_classification
    else:
        return current_classification

auto_sctask_df['Auto_Classification_type'] = auto_sctask_df.apply(lambda x: DTS(x.dv_short_description, x.dv_workflow, x.Auto_Classification_type) if (x.dv_workflow.startswith("Tech-Store - Standard Hardware Item") & 
                                                              ("Complete" in x.dv_state)) else x.Auto_Classification_type, axis=1)


# In[113]:


Genesys = auto_sctask_df['Auto_Classification_type'].str.startswith("DTS Genesys")
auto_sctask_df.loc[Genesys, 'Activity_count'] = str(1)


# In[114]:


#auto_sctask_df.to_excel('fileout_Exceptions.xlsx')


# In[115]:
# Now combining the rows which were removed earlier from the data frame which has to be categiorised under Tech Vend Onsite(df_TV)
# Create a new DataFrame with the same columns as df, filled with empty strings for missing columns
df_filled = pd.DataFrame(columns=auto_sctask_df.columns).fillna('')

# Append df1 to the new DataFrame, filling missing columns with empty strings
df_filled = df_filled.append(combined_df).fillna('')

# Concatenate the original DataFrame and the new DataFrame
task_df = pd.concat([auto_sctask_df, df_filled], ignore_index=True)

task_df

#### Eastablishing connection to spark
engine2=connect_to_spark()

with engine2.connect() as con:
    query='use ServiceNOW;'
    aa=con.execute(query)

    query_names="""
    DROP TABLE if exists ##tmp_u_leaver_device1;
    """
    bb=con.execute(query_names)

    query_names="""
    select lvd.description, lvd.number, lvd.short_description, lvd.dv_assignment_group, lvd.dv_parent, lvd.dv_u_substate as LVD_substate, lvr.dv_state as LVR_state, lvd.closed_at, lvd.sys_created_on, lvd.cmdb_ci, lvd.dv_u_asset, lvd.sys_id
    into ##tmp_u_leaver_device1
    from 
    [~u_leaver] lvr
    left join
    [~u_leaver_device] lvd
    on 
    lvd.parent=lvr.sys_id
    where
    u_leaving>convert(datetime,'2022-10-01 00:00:00',120)
    
    ;
    """
    bb=con.execute(query_names)
 
    query_names="""
    DROP TABLE if exists ##tmp_u_leaver_device2;
    """
    bb=con.execute(query_names)

    query_names="""
    select asset, model_id, lvd.*
    into ##tmp_u_leaver_device2 
    from ##tmp_u_leaver_device1 lvd
    left join 
    [~cmdb] cmdb
    on 
    cmdb.sys_id=lvd.cmdb_ci
    ;
    """
    bb=con.execute(query_names)
 
    query_names="""
    DROP TABLE if exists ##tmp_u_leaver_device3;
    """
    bb=con.execute(query_names)

    query_names="""
    select cm.dv_u_model_type, au.*
    into ##tmp_u_leaver_device3 
    from ##tmp_u_leaver_device2 au
    left join
    [~cmdb_model] cm
    on 
    cm.sys_id=au.model_id
    ;
    """
    
    
    
    
    bb=con.execute(query_names)
 
    query_names="""
    
    select * from
    ##tmp_u_leaver_device3
    ;
    """
    
    df_data_extract_leavers=pd.read_sql(query_names, con)
    
   

df_data_extract_leavers.columns

df_data_extract_leavers=df_data_extract_leavers.replace({pd.NaT: '', np.NaN: ''})

df_data_extract_leavers=df_data_extract_leavers.replace(to_replace={'dv_location': {'': 'Corporate'}})
df_data_extract_leavers['description'] = df_data_extract_leavers['description'].str.lower()
df_data_extract_leavers.loc[(df_data_extract_leavers['description'].astype(str).str.contains("monitor")),'dv_u_model_type']= 'monitor'

# Convert 'dv_u_model_type', 'LVR_state' column to lowercase
df_data_extract_leavers['dv_u_model_type'] = df_data_extract_leavers['dv_u_model_type'].str.lower()
df_data_extract_leavers['LVR_state'] = df_data_extract_leavers['LVR_state'].str.lower()

### Import Libraries(Regular Expression)
import re
def rev_model(dv_u_model_type):
    if 'mac' in dv_u_model_type:
        return('Mac')
    elif 'windows' in dv_u_model_type:
        return('Windows')
    elif 'monitor' in dv_u_model_type:
        return('Monitor')
    else:
        return('Other device')
    
df_data_extract_leavers['rev_model_type'] = df_data_extract_leavers.apply(lambda x: rev_model(x.dv_u_model_type) if ("closed complete" in x.LVR_state) else None, axis=1)



########## New change 
df = df_data_extract_leavers
# Group the data based on the 'dv_parent' column
grouped = df.groupby('dv_parent')


# Initialize an empty list to store DataFrames for groups with all rows as 'closed - asset updated'|'complete'
closed_groups = []

# Check each group for all rows as 'closed - asset updated','complete' and store in the list
#############Closed Complete,On Hold, Scheduling Required, In Progress

for name, group in grouped:
    #if all(group['LVR_state'].isin(['closed - asset updated','complete'])):
    if all(group['LVR_state'].isin(['closed complete'])):
    #all(group['dv_state'] == 'closed - asset updated'|'complete'):
        
        #group['max_date'] = group['closed_date'].max()
        closed_groups.append(group)
        
# Check if closed-asset,complete_groups list is empty
if not closed_groups:
    # Create an empty DataFrame as the result
    closed_req_df = pd.DataFrame(columns=df.columns)
else:
    # Concatenate the DataFrames in the list
    closed_req_df = pd.concat(closed_groups)

# Display the result DataFrame
closed_req_df

########################
# Convert date column to datetime format
closed_req_df['closed_date'] = pd.to_datetime(closed_req_df['closed_at'])

# Group the data by 'dv_parent'
grouped = closed_req_df.groupby('dv_parent')

# Find the most recent date in each group -max date of each is taken and all the rows in that group gets overriden by the max date 
most_recent_date = grouped['closed_date'].transform('max')

# Update the 'date' column with the most recent date for each group and convert to date format
closed_req_df['closed_date'] = most_recent_date.dt.date
closed_req_df

############## Here we are updated the yearmonth column values based on each group where the maximum date in the group is taken
closed_req_df['yearmonth'] = closed_req_df['closed_date'].apply(lambda x: 
    "{:04d}".format(int(x.year))+"{:02d}".format(int(x.month)) if isinstance(x, datetime) and not pd.isnull(x) 
    else ('190001' if (isinstance(x, str) and x=='') else "{:04d}".format(int(pd.to_datetime(x).year))+"{:02d}".format(int(pd.to_datetime(x).month)) if not pd.isnull(x) else ''))

closed_req_df.to_excel('fileout.xlsx')

closed_req_df['LVD_substate'] = closed_req_df['LVD_substate'].str.lower()

#leaver_win_df['LVD_substate'] = leaver_win_df['LVD_substate'].str.lower()
def classify_rows(row):
    if row['LVD_substate'] in ['collected', 'rebuilt', 'returned']:
        if row['rev_model_type'] in ['Windows', 'Monitor']:
            return pd.Series({'Auto_Classification_type': 'Multiple Leaver Device', 'Activity_count': 1})
        else:
            return pd.Series({'Auto_Classification_type': 'Duplicate', 'Activity_count': 0})

    else:
        return pd.Series({'Auto_Classification_type': 'Duplicate', 'Activity_count': 0})

result_df = closed_req_df.apply(classify_rows, axis=1)
closed_req_df['Auto_Classification_type'] = result_df['Auto_Classification_type']
closed_req_df['Activity_count'] = result_df['Activity_count']

def update_ld(group):
    for idx, row in group.iterrows():
        if 'collected' in row['LVD_substate'] or 'returned' in row['LVD_substate'] or 'rebuilt' in row['LVD_substate']:
            if row['Activity_count'] == 1:
                group.at[idx, 'Auto_Classification_type'] = 'Leaver Device Record'
                break  # Break out of the loop after updating
    return group


closed_req_df = closed_req_df.groupby('dv_parent').apply(update_ld)
closed_req_df.reset_index(drop=True, inplace=True)

closed_req_df
closed_req_df.to_excel('fileout.xlsx')

#df_leavers = df_leavers.replace(to_replace={'dv_u_completion_method': {'': 'Engineer - Desktop'}})


closed_req_df=closed_req_df.replace({pd.NaT: '', np.NaN: ''})
### Function to classify assignment group for leavers
def assign_business_group_leaver(x):
    result=corporate_string

    ass_group=x.dv_assignment_group
    
    if(ass_group=='UK CSG Desktop Support'):
        return csg_string
    if(ass_group == 'UK Desktop Support'):
        return corporate_string
    return result

### Calling the function for the Business group class(for leavers data)
closed_req_df[business_group_string]=closed_req_df.apply(lambda x: assign_business_group_leaver(x), axis=1)
## renaming column names to match with the existing ones
column_names_current=['number', 'dv_parent', 'closed_date', 'dv_assignment_group', 'short_description', 'description', 'dv_u_model_type', 'rev_model_type', 'dv_location', 'yearmonth', 'closed_date', 'LVD_substate', 'LVR_state', 'rev_model_type','dv_u_completion_method','Auto_Classification_type', 'Activity_count']
column_names_original=['number', 'dv_request', 'closed_date', 'dv_assignment_group', 'dv_short_description', 'description', 'dv_u_model_type', 'rev_model_type', 'Business_group', 'yearmonth', 'closed_date', 'LVD_substate', 'dv_state', 'rev_model_type','dv_u_completion_method','Auto_Classification_type', 'Activity_count']

for i in range(len(column_names_current)):
    #merged_df=merged_df.rename(columns={column_names_merge_extract[i]:column_names_original[i]})
    closed_req_df=closed_req_df.rename(columns={column_names_current[i]:column_names_original[i]})



###########################################################
# In[93]:




#df_leavers = df_leavers.replace(to_replace={'dv_u_completion_method': {'': 'Engineer - Desktop'}})

# Now combining the rows which were removed earlier from the data frame which has to be categiorised as DTS-Genesys  DTS_df.shape
# Create a new DataFrame with the same columns as df, filled with empty strings for missing columns
#df1_filled = pd.DataFrame(columns=auto_sctask_df.columns).fillna('')

# Append df1 to the new DataFrame, filling missing columns with empty strings
#df1_filled = df1_filled.append(DTS_df).fillna('')

# Concatenate the original DataFrame and the new DataFrame
#task_df = pd.concat([auto_sctask_df, df1_filled], ignore_index=True)

#task_df


# In[116]:


task_df=task_df.replace({pd.NaT: '', np.NaN: ''})


# In[117]:


task_df['closed_date'] = pd.DatetimeIndex(task_df['closed_at']).date


# In[118]:


# Now combining the rows which were removed earlier from the data frame which has to be categiorised as DTS-Genesys  DTS_df.shape
# Create a new DataFrame with the same columns as df, filled with empty strings for missing columns
#df1_filled_OM = pd.DataFrame(columns=task_df.columns).fillna('')

# Append df1 to the new DataFrame, filling missing columns with empty strings
#df1_filled_OM = df1_filled_OM.append(OM_df).fillna('')

# Concatenate the original DataFrame and the new DataFrame
#task_OM_df = pd.concat([task_df, df1_filled_OM], ignore_index=True)

#task_OM_df


# In[119]:

##################################################################################################################
# Create a new DataFrame with the same columns as df, filled with empty strings for missing columns
df2_filled = pd.DataFrame(columns=task_df.columns).fillna('')

# Append df1 to the new DataFrame, filling missing columns with empty strings
df2_filled = df2_filled.append(closed_req_df).fillna('')

# Concatenate the original DataFrame and the new DataFrame
dts_sctasks_df = pd.concat([task_df, df2_filled], ignore_index=True)

dts_sctasks_df
dts_sctasks_df.to_excel('fileoutcc.xlsx')

### Just for reference we need to have a column LVD_substate to check the state of LVD's so joining a column from the closed_req_df


#dts_sctasks_df.replace({pd.NaT: '', np.NaN: ''})


# In[121]:


dts_sctasks_df = dts_sctasks_df.replace(to_replace={'Completion_method': {'': 'Engineer - Desktop'}})
### Change the classification names in the Auto_Classification_type column to the format required for matching in TABLE5
dts_sctasks_df=dts_sctasks_df.replace(to_replace={'Auto_Classification_type': {'NS_hardware_bundle': 'Hardware Bundle New Starter', 'hardware_bundle': 'Hardware Bundle', 'peripheral_bundle': 'Peripheral Bundle', 'consumable_bundle': 'Consumable Bundle' } })


# In[131]:


mask = (dts_sctasks_df['dv_workflow'].str.contains('Tech-Store - Standard Hardware Item', case=False)) & \
       (dts_sctasks_df['dv_u_model_type'].str.contains('laptop|Desktop|workStation', case=False)) & \
       (dts_sctasks_df['description'].str.contains('Change of Role/Responsibility', case=False)) & \
       (~dts_sctasks_df['dv_short_description'].str.contains('DTS Genesys|Order Management', case=False))

dts_sctasks_df.loc[mask, 'Auto_Classification_type'] = 'Hardware Bundle New Starter'

dts_sctasks_df


# In[132]:


#dts_sctasks_df.to_excel('fileout_Exceptions.xlsx')


# In[133]:


#dts_sctasks_df.to_excel('fileout_Exceptions.xlsx')


# In[134]:


### For tasks having 'Change of roles' request.
### Replacing the regular bundle calculation to New Starter calculations, if 'Change of Role' appears in the 'description' column
#for #i in range(len(dts_sctasks_df['description'])):
    #if 'Change of Role/Responsibility' in dts_sctasks_df['description'][i]:
    #   dts_sctasks_df['Auto_Classification_type'][i] = 'Hardware Bundle New Starter'


# In[135]:


# Replace all empty string with duplicates in the Auto_Classification_type column

dts_sctasks_df['Auto_Classification_type'] = dts_sctasks_df['Auto_Classification_type'].replace('', 'Duplicate')


# In[136]:


#check# drop unnecessary column dv_u_completion_method	closed_at
#dts_sctasks_df = dts_sctasks_df.drop(['closed_at', 'dv_u_completion_method'], axis=1)
#dts_sctasks_df


# ### Leavers data

# In[137]:


#auto_sctask_df


# In[138]:


#dts_sctasks_df.to_excel('fileout_Exceptions.xlsx')


# In[139]:


dts_sctasks_df=dts_sctasks_df.replace({pd.NaT: '', np.NaN: ''})
dts_sctasks_df


# In[140]:


dts_sctasks_df['closed_date'] = pd.DatetimeIndex(dts_sctasks_df['closed_at']).date


# In[141]:
column_subset_extract4 = ['number', 'dv_request', 'dv_request_item', 'closed_date', 'dv_assignment_group', 'user_location','dv_u_model_type', 'dv_workflow', 'quantity', 'dv_short_description', 'description',
                         'dv_u_completion_method', 'dv_u_completion_category', 'dv_state', 'LVD_substate', 'Completion_method', 'Auto_Classification_type', 'Activity_count', 'Business_group', 'assign_classification', 'yearmonth']

#column_subset_extract4 = ['number', 'dv_request', 'dv_request_item', 'closed_date', 'dv_assignment_group', 'user_location','dv_u_model_type', 'dv_workflow', 'quantity', 'dv_short_description', 'description',
                         #'dv_u_completion_method', 'dv_u_completion_category', 'dv_state', 'LVD_substate', 'Completion_method', 'Auto_Classification_type', 'Activity_count', 'Business_group', 'assign_classification', 'yearmonth', 'Auto_Classification_Override', 'Activity_count_Override', 'Completion_method_Override']
dts_sctasks_df=dts_sctasks_df[column_subset_extract4]
# Once we get the manual Exception handling completed then we write the complete data along with manual override to datawarehouse for future reference.
# for the completion method where ever there is an empty string allot it as an engineer desktop Before mapping it back to check the complexity simple, moderate, complex
dts_sctasks_df.to_excel('fileout.xlsx')

########################## For additional column which provides the count of RITM'S associated with REQ'S (One to many)
# Create a new column 'occurence' with the occurrence count of values in column 'dv_request'
dts_sctasks_df['occurrence'] = dts_sctasks_df.groupby('dv_request')['dv_request'].transform('count')

# Sort the resulting df by the 'dv_request' column to keep rows with the same group together
dts_sctasks_df = dts_sctasks_df.sort_values(by='dv_request')

### The data is exported to the data ware house 

engine=connect_to_data_warehouse()

with engine.connect() as con:
    query='use DataWarehouse;'
    aa=con.execute(query)
    query_names="""
    DROP TABLE IF EXISTS DTS_tasks;
    """
    bb=con.execute(query_names)
    #dts_sctasks_df.iloc[7209][:4].to_sql('DTS_tasks', con,chunksize=1, if_exists='replace', index=False)
    #dts_sctasks_df.tail(6000).to_sql('DTS_tasks', con,chunksize=2000, if_exists='replace', index=False)
    dts_sctasks_df.to_sql('DTS_tasks', con,chunksize=2000, if_exists='replace')


df_datawarehouse=dts_sctasks_df
query_cat='''
select UPPER(Table_5_Activity_Point_Classification) as Auto_Classification_type_UPPERCASE, Category
from
DTS_T5_Activity_Points_Revised
;
'''


# In[20]:


engine=connect_to_data_warehouse()

from sqlalchemy.orm import Session
from sqlalchemy import Table
result=execute_query(engine, "use BusOpsDataWarehouse;")

with Session(engine) as session:
    df_categories=pd.read_sql(query_cat,session.bind)
    
#  to remove spaces
df_datawarehouse['Auto_Classification_type_UPPERCASE']=df_datawarehouse['Auto_Classification_type'].str.replace(" ", "")
# to avoid case sensitivity - make uppercase
df_datawarehouse['Auto_Classification_type_UPPERCASE']=df_datawarehouse['Auto_Classification_type_UPPERCASE'].str.upper()

df_categories['Auto_Classification_type_UPPERCASE']=df_categories['Auto_Classification_type_UPPERCASE'].str.replace(" ", "")
df_categories.head()


df_merged=pd.merge(df_datawarehouse, df_categories,on='Auto_Classification_type_UPPERCASE', how='left')
df_merged=df_merged.drop(columns='Auto_Classification_type_UPPERCASE')




df_merged=df_merged.fillna('')

#*********************#
# here add manual override

query_overridden_tasks='''

select mo.number,mo.Auto_Classification_Override, mo.Activity_count_Override, mo.Completion_method_Override, points.Category as override_category
from

DTS_tasks_manual_override mo
left join
DTS_T5_Activity_Points_Revised points
on
mo.Auto_Classification_Override=points.Table_5_Activity_Point_Classification
;
'''

# here take info on manual override
engine=connect_to_data_warehouse()

from sqlalchemy.orm import Session
from sqlalchemy import Table
result=execute_query(engine, "use DataWarehouse;")

with Session(engine) as session:
    df_manual_override=pd.read_sql(query_overridden_tasks,session.bind).fillna('')
    


df_merged['Override_Auto_Classification']=''
df_merged['Override_Activity_Count']=''
df_merged['Override_Completion_Method']=''

def update_classifications(row):
    if pd.notna(row['Auto_Classification_Override']) and row['Auto_Classification_Override'] != '':
        row['Override_Auto_Classification'] = row['Auto_Classification_Override']
        row['Auto_Classification_type'] = row['Auto_Classification_Override']
        row['Override_Activity_Count'] = row['Activity_count_Override']
        row['Activity_count'] = row['Activity_count_Override']
        row['Override_Completion_Method'] = row['Completion_method_Override']
        row['Completion_method'] = row['Completion_method_Override']
        row['Category'] = row['override_category'].strip()
        #row['Category'] = row['override_category']
    return row

# There are manual overrides
if len(df_manual_override) > 0:
    df_merged = pd.merge(df_merged, df_manual_override, on='number', how='left')
    df_merged = df_merged.apply(update_classifications, axis=1)
    #df_merged.drop(['Auto_Classification_Override','override_category'], axis=1, inplace=True)
    df_merged.drop(['Auto_Classification_Override', 'Activity_count_Override', 'Completion_method_Override', 'override_category'], axis=1, inplace=True)



####  Points classification based on Completion Method
def categorize_points(cat_string, delivery):
    lower = cat_string.lower()
    lower_delivery = delivery.lower()
    
    ret_val = 0
    
    if lower == 'simple':
        ret_val = 1
    elif lower == 'moderate':
        ret_val = 3
    elif lower == 'complex':
        ret_val = 6
    
    #Delivery Only
    if lower_delivery == 'delivery only':
        if ret_val == 1:
            ret_val = 1
        elif ret_val == 3:
            ret_val = 1
        elif ret_val == 6:
            ret_val = 3
        return ret_val
    
    #Engineer - Desktop
    if lower_delivery == 'engineer - desktop':
        if ret_val == 1:
            ret_val = 1
        elif ret_val == 3:
            ret_val = 3
        elif ret_val == 6:
            ret_val = 6
        return ret_val
    
    #Engineer - Field 
    if lower_delivery == 'engineer - field':
        if ret_val == 1:
            ret_val = 3
        elif ret_val == 3:
            ret_val = 6
        elif ret_val == 6:
            ret_val = 6
        return ret_val
    
    #Delivery & Collection (Courier)
    if lower_delivery == 'delivery & collection (courier)':
        if ret_val == 1:
            ret_val = 1
        elif ret_val == 3:
            ret_val = 1
        elif ret_val == 6:
            ret_val = 3
        return ret_val
    
    
    #Remote Connection
    if lower_delivery == 'remote connection':
        if ret_val == 1:
            ret_val = 1
        elif ret_val == 3:
            ret_val = 1
        elif ret_val == 6:
            ret_val = 3
        return ret_val
    
    return ret_val


# In[159]:


df_merged['Points_classification']=df_merged.apply(lambda x: categorize_points(x['Category'], x['Completion_method']),axis=1)


# In[160]:


df_merged


# In[161]:


df_merged.dtypes


# In[162]:


#df_merged = df_merged.drop('Category', axis=1)
#df_merged = df_merged.drop('Unnamed: 0')


# In[163]:


#df_merged['Activity_count'] = df_merged['Activity_count'].replace('Exception', 0)


# In[164]:


#df_merged.dtypes


# In[165]:


for index, value in df_merged['Activity_count'].items():
    if not str(value).isnumeric():
        print(f"Non-numeric value found at index {index}: {value}")


# In[166]:


# convert 'Activity_count' column to numeric data type, replace non-numeric values with NaN

df_merged['Activity_count'] = pd.to_numeric(df_merged['Activity_count'], errors='coerce')

# replace NaN values with 0
df_merged['Activity_count'] = df_merged['Activity_count'].fillna(0)

# convert 'Activity_count' column to integer data type
df_merged['Activity_count'] = df_merged['Activity_count'].astype(int)

# print the updated DataFrame with data types
print(df_merged.dtypes)


# In[167]:


for index, value in df_merged['Activity_count'].items():
    if not str(value).isnumeric():
        print(f"Non-numeric value found at index {index}: {value}")


# In[168]:


#df_merged.to_excel('fileout1.xlsx')


# In[169]:


#df_merged


# In[170]:


#df_merged.dtypes


# In[171]:


###### Get the Final Point Classification by multiplying 'Activity_count' column with 'Points_classification' column
df_merged['Final_Point_Classification'] = (df_merged['Activity_count'] * df_merged['Points_classification'])


# In[172]:


#df_merged


# In[173]:


#df_merged.to_excel('fileout_category.xlsx')


# In[174]:



def save_dataframe(filename,df, sheet_name, savemode=None):
    # create a excel writer object as shown using
    # Excelwriter function
    
    from pathlib import Path
    
    if_sheet_exists=None

    my_file = Path(filename)
    if(savemode is None):
        if (my_file.is_file()):
            savemode='a'
            if_sheet_exists='replace'
        else:
            savemode='w'
    
    with pd.ExcelWriter(filename, mode=savemode, engine="openpyxl", if_sheet_exists=if_sheet_exists ) as writer:
         
        # use to_excel function and specify the sheet_name and index to
        # store the dataframe in specified sheet
        df.to_excel(writer, sheet_name=sheet_name)


# In[175]:

####################################################################################
# Defining functions
def save_volume_summary_cp(filename, df_merged, b_group):
    b_group='Corporate'
    pv_tb=pd.pivot_table(df_merged.where(df_merged['Business_group']==b_group),values='Activity_count', index=['Auto_Classification_type','Category','Completion_method'],columns='yearmonth',aggfunc=len, fill_value=0, margins=True, margins_name='Corporate_Vol_Total')
    pv_tb = pv_tb.iloc[:, :-1]                                                                          #index=['Auto_Classification_type','Category','Completion_method'],columns='yearmonth',aggfunc=sum, fill_value=0)

    save_dataframe(filename,pv_tb, sheet_name="Vol"+b_group+"_wdelivery")
    


# In[ ]:


## to get the total of all columns for the sum of count of each classification
def save_volume_summary_tot_cor(filename, df_merged, b_group):
    b_group='Corporate'
    pv_tb=pd.pivot_table(df_merged.where(df_merged['Business_group']==b_group), values='Activity_count', index=['Auto_Classification_type'], columns=['yearmonth'], aggfunc=np.sum, fill_value=0)
    
    # Add a row for the total of each column, excluding 'Duplicate' values in the 'Class' column
    total_row = pd.DataFrame(pv_tb.loc[pv_tb.index != 'Duplicate'].sum(axis=0), columns=['Corporate_Activity_Total']).T
    pv_tb = pd.concat([pv_tb, total_row])
    save_dataframe(filename,pv_tb, sheet_name="Vol"+b_group+"Total")
    
    


# In[ ]:

def save_volume_summary_cg(filename, df_merged, b_group):
    b_group='CSG'
    pv_tb=pd.pivot_table(df_merged.where(df_merged['Business_group']==b_group),values='Activity_count', index=['Auto_Classification_type','Category','Completion_method'],columns='yearmonth',aggfunc=len, fill_value=0, margins=True, margins_name='CSG_Vol_Total')
    #pv_tb=pv_tb.reindex(sorted(pv_tb.columns), axis=1)                                                  #index=['Auto_Classification_type','Category','Completion_method'],columns='yearmonth',aggfunc=sum, fill_value=0)

    pv_tb = pv_tb.iloc[:, :-1]
    save_dataframe(filename,pv_tb, sheet_name="Vol"+b_group+"_wdelivery")
    


# In[ ]:


## to get the total of all columns for the sum of count of each classification
def save_volume_summary_tot_csg(filename, df_merged, b_group):
    b_group='CSG'
    pv_tb=pd.pivot_table(df_merged.where(df_merged['Business_group']==b_group), values='Activity_count', index=['Auto_Classification_type'], columns=['yearmonth'], aggfunc=np.sum, fill_value=0)
    
    # Add a row for the total of each column, excluding 'Duplicate' values in the 'Class' column
    total_row = pd.DataFrame(pv_tb.loc[pv_tb.index != 'Duplicate'].sum(axis=0), columns=['CSG_Activity_Total']).T
    pv_tb = pd.concat([pv_tb, total_row])
    save_dataframe(filename,pv_tb, sheet_name="Vol"+b_group+"Total")
    


# In[ ]:

def save_points_summary_cp(filename, df_merged, b_group):
    b_group='Corporate'
    pv_tb=pd.pivot_table(df_merged.where(df_merged['Business_group']==b_group),values='Final_Point_Classification', index=['Auto_Classification_type','Category','Completion_method'],columns='yearmonth',aggfunc=sum, fill_value=0, margins=True, margins_name='Corporate_Points_Total')
    pv_tb = pv_tb.iloc[:, :-1]
    save_dataframe(filename,pv_tb, sheet_name="Points"+b_group+"_wdelivery")

def save_points_summary_total_cp(filename, df_merged, b_group):
    b_group='Corporate'
    pv_tb=pd.pivot_table(df_merged.where(df_merged['Business_group']==b_group),values='Final_Point_Classification', index=['Auto_Classification_type'],columns='yearmonth',aggfunc=sum, fill_value=0, margins=True, margins_name='Corporate_Points_Total')
    pv_tb = pv_tb.iloc[:, :-1]
    save_dataframe(filename,pv_tb, sheet_name="Points"+b_group+"_wdelivery")

    


# In[ ]:

def save_points_summary_cg(filename, df_merged, b_group):
    b_group='CSG'
    pv_tb=pd.pivot_table(df_merged.where(df_merged['Business_group']==b_group),values='Final_Point_Classification', index=['Auto_Classification_type','Category','Completion_method'],columns='yearmonth',aggfunc=sum, fill_value=0, margins=True, margins_name='Corporate_Points_Total')
    pv_tb = pv_tb.iloc[:, :-1]
    save_dataframe(filename,pv_tb, sheet_name="Points"+b_group+"_wdelivery")
    
def save_points_summary_total_cg(filename, df_merged, b_group):
    b_group='CSG'
    pv_tb=pd.pivot_table(df_merged.where(df_merged['Business_group']==b_group),values='Final_Point_Classification', index=['Auto_Classification_type'],columns='yearmonth',aggfunc=sum, fill_value=0, margins=True, margins_name='CSG_Points_Total')
    pv_tb = pv_tb.iloc[:, :-1]
    save_dataframe(filename,pv_tb, sheet_name="Points"+b_group+"_wdelivery")
    


# In[ ]:

def save_volume_summary_total(filename, df_merged):
    #b_group='Corporate'
    pv_tb=pd.pivot_table(df_merged,values='Activity_count', index=['Business_group'],columns='yearmonth',aggfunc=np.sum, fill_value=0, margins=True, margins_name='Volume_Total')
    pv_tb = pv_tb.iloc[:, :-1]                                                                          #index=['Auto_Classification_type','Category','Completion_method'],columns='yearmonth',aggfunc=sum, fill_value=0)

    save_dataframe(filename,pv_tb, sheet_name="Volume_Total")

                   
def save_points_summary_total(filename, df_merged):
    #b_group='Corporate'
    pv_tb=pd.pivot_table(df_merged, values='Final_Point_Classification', index=['Business_group'],columns='yearmonth',aggfunc=np.sum, fill_value=0, margins=True, margins_name='Points_Total')
    pv_tb = pv_tb.iloc[:, :-1]
    save_dataframe(filename,pv_tb, sheet_name="Points_Total")
    


import glob
import shutil
import os

file_path = data_warehouse_dir_path+filename_tasks+"*.xlsx"
files = glob.glob(file_path)
for file in files:
    try:
        os.remove(file)
    except Exception as e:
        print('For file ', file, ' the following error occurred:')
        print(e)

#fullpath=data_warehouse_dir_path+filename_incidents+"_{:%Y_%m_%d}.xlsx".format(datetime.now())

fullpath=data_warehouse_dir_path+filename_tasks+"_{:%Y_%m_%d__H_%H_%M_%S}.xlsx".format(datetime.now())

try:
    
### Save the analysed data back in Excel sheet 
#filename_tasks='TASKS_Reporting'

#fullpath=data_warehouse_dir_path+filename_tasks+"_{:%Y_%m_%d}.xlsx".format(datetime.now())

    df_merged.to_excel(fullpath, sheet_name='Analysed_Data', index=False)
    
    save_volume_summary_total(filename=fullpath, df_merged=df_merged)
    save_points_summary_total(filename=fullpath, df_merged=df_merged)

    save_volume_summary_tot_cor(filename=fullpath, df_merged=df_merged, b_group='Corporate')
    save_points_summary_total_cp(filename=fullpath, df_merged=df_merged, b_group='Corporate')

    save_volume_summary_tot_csg(filename=fullpath, df_merged=df_merged, b_group='CSG')
    save_points_summary_total_cg(filename=fullpath, df_merged=df_merged, b_group='CSG')


except Exception as e:
    print('For file ', fullpath, ' the following error occurred:')
    print(e)


print('Tasks reporting successfully completed!!!!')
##############################################################################

